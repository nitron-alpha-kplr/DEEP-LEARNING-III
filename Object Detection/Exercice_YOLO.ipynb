{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitron-alpha-kplr/DEEP-LEARNING-III/blob/main/Object%20Detection/Exercice_YOLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reconnaissance des plaques d'immatriculation à l'aide de YoLo et DarkNet**"
      ],
      "metadata": {
        "id": "9opVMK6RKrHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://betterdatascience.com/detect-license-plates-with-yolo/images/1.jpg'>\n",
        "\n"
      ],
      "metadata": {
        "id": "cc0nQblznWeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Darknet :**"
      ],
      "metadata": {
        "id": "5e-2KC_QofpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://miro.medium.com/v2/resize:fit:512/0*GFsAY18k3A25IpRL'>"
      ],
      "metadata": {
        "id": "CmPifM2wot2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Darknet est un framework de réseau de neurones développé par Joseph Chet Redom.\n",
        "- Il s'agit essentiellement de nous fournir à tous un terrain de jeu pour construire nos propres modèles de réseaux de neurones.\n",
        "- Ce framework est écrit en « C » et « CUDA ».\n",
        "- Il est rapide et prend en charge les calculs CPU et GPU.\n",
        "-  Ce framework dépend des packages OpenCV et CUDA. Il prend en charge une large gamme de modèles tels que YOLO, RNN, etc.\n",
        "- Pour plus d'informations sur Darknet, vous pouvez visiter ce site Web https://pjreddie.com/darknet/ ."
      ],
      "metadata": {
        "id": "S1-DhzNIpR-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLO _ V3 :**"
      ],
      "metadata": {
        "id": "0djjVef2pgGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- YOLO signifie Y ou O nly L ook O nce. C'est l'un des algorithmes avancés actuels lorsque vous avez besoin d'une détection en temps réel (en millisecondes) de la classe et de la prédiction de la classe de cet objet détecté.\n",
        "- D'autre part, ce n'est pas l'algorithme le plus précis, mais c'est le meilleur des modèles de détection précis en temps réel.\n",
        "- Avant la version, nous avions les modèles yolo_v1 et yolo_v2. vous pouvez obtenir plus de détails dans leurs documents officiels publiés. yolo_v1 et yolo_v2 .\n",
        "\n",
        "- J'essaie d'expliquer un peu l'architecture v3. Il a un total de 106 couches. - Le meilleur avantage de ce modèle est qu'il fait des prédictions à trois échelles ( à la couche - 82 , 94 , 106 ).\n",
        "- Et en même temps, les images seront sous-échantillonnées leurs dimensions à 52 X 52 , 26X26 , 13X13 respectivement dans ces couches. Peu d'avantages sont\n",
        "\n",
        "1 .\n",
        "- Les caractéristiques sont détectées en multi-échelle (52 x 52 , 26 x 26 , 13x13 ).\n",
        "\n",
        "2. Réseau d'extracteurs de fonctionnalités plus puissant\n",
        "\n",
        "3. Fonction de perte optimisée\n",
        "\n",
        "- L'image suivante vous donne un aperçu plus détaillé de toutes les couches.\n",
        "\n"
      ],
      "metadata": {
        "id": "kYnbIS3Vpi-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/0*3v10JDBF-1CApcQV.png'>"
      ],
      "metadata": {
        "id": "jBQ4DtKmpy3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- À haut niveau, les couches ci-dessus peuvent être divisées en deux catégories principales - Feature Extractor et Detector.\n",
        "- Lorsqu'une nouvelle image arrive dans le réseau, elle passe d'abord par l'extracteur de caractéristiques (les caractéristiques sortiront à trois étapes différentes à trois échelles différentes), puis ces caractéristiques sont introduites dans la branche du détecteur pour détecter les boîtes englobantes autour des objets détectés et enfin ces boîtes englobantes seront classées dans l'une des catégories."
      ],
      "metadata": {
        "id": "A1tZ3TpBqAVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/0*RTdjlhEkAsFBMsU4.jpeg'>"
      ],
      "metadata": {
        "id": "2BM_4tj2qJoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracteur de fonctionnalités :**"
      ],
      "metadata": {
        "id": "GbA9wmwoqdV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- La fonctionnalité extracor dans YOLO V3 (également appelée Darknet -53) comporte elle-même 53 couches distinctes (sur un total de 106 couches).\n",
        "- Ces 53 couches de Darknet peuvent être expliquées avec le tableau suivant.\n",
        "<img src='https://miro.medium.com/v2/resize:fit:640/format:webp/0*lKqQiOgaDBjpjcZo.jpeg'>"
      ],
      "metadata": {
        "id": "275WqQuCqfTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- L'ensemble total du cadre peut être divisé en petits blocs indépendants qui sont connectés avec une couche de 2 convolutions.\n",
        "- Nous pouvons directement convertir cet extracteur de caractéristiques en classificateur en ajoutant trois couches distinctes (Avg-pool Connected et Soft-max).\n",
        "- Mais dans notre application de détection d'objets, nous utilisons ce Darknet-53 comme seul extracteur de fonctionnalités et est connecté à un bloc de couches de détection.\n",
        "- Nous n'inclurons donc pas les trois dernières couches dans notre modèle.\n",
        "\n",
        "- À partir de cet extracteur de caractéristiques, si nous insérons une image, cela donnera trois vecteurs de caractéristiques à différentes étapes du flux de réseau.\n",
        "- Ces trois fonctionnalités seront insérées séparément dans le bloc Détecteur."
      ],
      "metadata": {
        "id": "QqMVK-ApquqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importer la data depuis Kaggle**"
      ],
      "metadata": {
        "id": "T9QWeQ_-NIJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK8TRn_E6iy4"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AqUby-W8Spj",
        "outputId": "ce95a2e4-b54c-49df-e4ff-aae2d56a37d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "njeIZLvm8Vch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "er_lJoOa8a6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d elysian01/car-number-plate-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLOipWVw8g3z",
        "outputId": "cc99bbcd-fe1c-4d59-ca77-5bd5dfec5a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading car-number-plate-detection.zip to /content\n",
            "100% 2.69G/2.69G [01:40<00:00, 33.2MB/s]\n",
            "100% 2.69G/2.69G [01:40<00:00, 28.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d achrafkhazri/yolo-weights-for-licence-plate-detector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AdqnKpK9qHr",
        "outputId": "2ed1a3de-79e7-487e-ed32-bf7952ebf88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading yolo-weights-for-licence-plate-detector.zip to /content\n",
            " 99% 216M/218M [00:08<00:00, 24.3MB/s]\n",
            "100% 218M/218M [00:08<00:00, 25.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importer les Bibliothèques**"
      ],
      "metadata": {
        "id": "lLimpIbUNQdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "Drqi3G-T9xQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extraction des fichiers ZIP**"
      ],
      "metadata": {
        "id": "HJcfQr1rNWFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spécifiez le chemin du fichier ZIP\n",
        "zip_file_path = '/content/car-number-plate-detection.zip'\n",
        "# Spécifiez le répertoire où vous souhaitez extraire les fichiers\n",
        "extract_directory = '/content/car-number-plate-detection'\n",
        "# Ouvrez le fichier ZIP\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extrayez tous les contenus du fichier ZIP dans le répertoire spécifié\n",
        "    zip_ref.extractall(extract_directory)\n",
        "print(\"Extraction terminée.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMfS3NgT-GgR",
        "outputId": "d6bb4b11-8b23-42f0-f3c6-265344b902b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spécifiez le chemin du fichier ZIP\n",
        "zip_file_path = '/content/yolo-weights-for-licence-plate-detector.zip'\n",
        "# Spécifiez le répertoire où vous souhaitez extraire les fichiers\n",
        "extract_directory = '/content/yolo-weights-for-licence-plate-detector'\n",
        "# Ouvrez le fichier ZIP\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extrayez tous les contenus du fichier ZIP dans le répertoire spécifié\n",
        "    zip_ref.extractall(extract_directory)\n",
        "print(\"Unzipping complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3TaUQca-TCQ",
        "outputId": "6cceb07c-437f-4160-ead7-eddfec0a97d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parcours récursif d'un répertoire et récupération des chemins de fichiers**"
      ],
      "metadata": {
        "id": "O9DhovnmNx6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Liste pour stocker les chemins des fichiers\n",
        "paths = []\n",
        "\n",
        "# Parcourir récursivement le répertoire '/content/car-number-plate-detection/Car_Number_Plate'\n",
        "# et récupérer les chemins de tous les fichiers\n",
        "for dirname, _, filenames in os.walk('/content/car-number-plate-detection/Car_Number_Plate'):\n",
        "    for filename in filenames:\n",
        "        # Ajouter le chemin complet du fichier à la liste des chemins\n",
        "        paths.append(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "PVaNjgCy9ygU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=len(paths)\n",
        "N=6"
      ],
      "metadata": {
        "id": "RDwi9jAj_LiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = open('/content/yolo-weights-for-licence-plate-detector/classes.names').read()"
      ],
      "metadata": {
        "id": "WIrJHlKI_N14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin vers les poids du modèle YOLO\n",
        "weights_path = '/content/yolo-weights-for-licence-plate-detector/lapi.weights'\n",
        "# Chemin vers la configuration du modèle YOLO\n",
        "configuration_path = '/content/yolo-weights-for-licence-plate-detector/darknet-yolov3.cfg'\n",
        "# Probabilité minimum pour filtrer les détections\n",
        "probability_minimum = 0.4\n",
        "# Seuil pour la suppression des détections faibles\n",
        "threshold = 0.3"
      ],
      "metadata": {
        "id": "Y4cxJMbQ_W5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuration du modèle YOLO et paramètres de détection**"
      ],
      "metadata": {
        "id": "zmDFpslWN-e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le modèle Darknet à partir de la configuration et des poids\n",
        "network = cv2.dnn.readNetFromDarknet(configuration_path, weights_path)\n",
        "\n",
        "# Récupérer les noms de toutes les couches du réseau\n",
        "layers_names_all = network.getLayerNames()\n",
        "\n",
        "# Récupérer les indices des couches de sortie non connectées\n",
        "layers_indices_output = network.getUnconnectedOutLayers()\n",
        "\n",
        "# Vérifier si layers_indices_output est un entier\n",
        "if isinstance(layers_indices_output, int):\n",
        "    # Convertir en liste\n",
        "    layers_indices_output = [layers_indices_output]\n",
        "\n",
        "# Récupérer les noms des couches de sortie à partir des indices\n",
        "layers_names_output = [layers_names_all[i - 1] for i in layers_indices_output]"
      ],
      "metadata": {
        "id": "O5pWT0gY_ity"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chargement et affichage de l'image**"
      ],
      "metadata": {
        "id": "jx5gC8Z4OZn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lire l'image à partir du chemin spécifié\n",
        "image_input = cv2.imread(paths[N])\n",
        "\n",
        "# Redimensionner l'image en utilisant un facteur de réduction de 0.2\n",
        "image_input = cv2.resize(image_input, dsize=None, fx=0.2, fy=0.2)\n",
        "\n",
        "# Afficher l'image en utilisant Matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configurer la taille de la figure\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "\n",
        "# Afficher l'image convertie en espace de couleur RGB\n",
        "plt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C159F9yS_8Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préparation de l'image pour la détection avec YOLO**"
      ],
      "metadata": {
        "id": "cYL_WuYTQrKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice : Convertir une image en blob d'entrée pour le réseau YOLO**\n",
        "\n",
        "- Dans cet exercice, nous allons convertir une image en un blob d'entrée pour le réseau YOLO en utilisant OpenCV.\n",
        "- Le blob d'entrée est une représentation prétraitée de l'image qui peut être utilisée pour effectuer des prédictions d'objets avec le réseau YOLO.\n",
        "\n",
        "- Voici le code à compléter :"
      ],
      "metadata": {
        "id": "QkYSRjO2swd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "# Lire l'image à partir du chemin spécifié\n",
        "image_input = cv2.imread(\"chemin/vers/l/image.jpg\")\n",
        "# Convertir l'image en un blob d'entrée pour le réseau YOLO\n",
        "blob = cv2.dnn.blobFromImage(#fill_here, #fill_here, #fill_here, #fill_here, #fill_here)\n",
        "# Réorganiser les dimensions du blob pour l'affichage\n",
        "blob_to_show = blob[0,:,:,:].transpose(#fill_here)\n",
        "# Définir le blob en tant qu'entrée pour le réseau\n",
        "network.setInput(#fill_here)\n",
        "# Obtenir les sorties du réseau pour les couches de sortie spécifiées\n",
        "output_from_network = network.forward(#fill_here)\n",
        "# Générer des couleurs aléatoires pour chaque étiquette de classe\n",
        "np.random.seed(42)\n",
        "colours = np.random.randint(#fill_here, #fill_here, size=(#fill_here, #fill_here), dtype='uint8')\n"
      ],
      "metadata": {
        "id": "oaPV9A5AAAMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions :**\n",
        "1.  Utilisez la méthode cv2.dnn.blobFromImage() pour convertir l'image en un blob d'entrée pour le réseau YOLO. Utilisez les arguments appropriés pour effectuer la conversion. Le facteur d'échelle (scalefactor) est 1/255.0, la taille du blob (size) est (416, 416), swapRB est True et crop est False. Assignez le résultat à la variable blob.\n",
        "2.  Réorganisez les dimensions du blob pour l'affichage en utilisant la méthode transpose() de l'objet blob. Les axes à transposer sont 0, 1 et 2, dans cet ordre. Assignez le résultat à la variable blob_to_show.\n",
        "3.  Définissez le blob en tant qu'entrée pour le réseau en utilisant la méthode setInput() de l'objet network. Passez le blob en tant qu'argument.\n",
        "4.  Obtenez les sorties du réseau pour les couches de sortie spécifiées en utilisant la méthode forward() de l'objet network. Passez les noms des couches de sortie (layers_names_output) en tant qu'argument. Assignez le résultat à la variable output_from_network.\n",
        "5.  Générez des couleurs aléatoires pour chaque étiquette de classe en utilisant la méthode random.randint() de l'objet np. Les arguments pour randint() sont la valeur minimale (low), la valeur maximale (high), la taille (size) de la matrice de couleurs et le type de données (dtype) en tant que 'uint8'. Utilisez np.random.seed(42) pour fixer la graine aléatoire."
      ],
      "metadata": {
        "id": "tlFfr5cTuAWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Détection d'objets avec YOLO et récupération des résultats**"
      ],
      "metadata": {
        "id": "aVJsT55uQ53S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice : Traitement des résultats de détection avec YOLO**\n",
        "\n",
        "- Dans cet exercice, nous allons traiter les résultats de détection obtenus à partir du réseau YOLO.\n",
        "- Nous allons extraire les boîtes englobantes, les confiances et les numéros de classe associés à chaque détection."
      ],
      "metadata": {
        "id": "q--Q-F5uwzKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Listes pour stocker les boîtes englobantes, les confiances et les numéros de classe\n",
        "bounding_boxes = []\n",
        "confidences = []\n",
        "class_numbers = []\n",
        "\n",
        "# Récupérer les dimensions de l'image\n",
        "h, w = image_input.shape[:2]\n",
        "\n",
        "# Parcourir les résultats de la sortie du réseau\n",
        "for result in output_from_network:\n",
        "    # Parcourir les détections dans chaque résultat\n",
        "    for detection in result:\n",
        "        # Récupérer les scores de confiance pour chaque classe\n",
        "        scores = detection[#fill_here_:]\n",
        "\n",
        "        # Trouver l'indice de classe avec la plus haute probabilité\n",
        "        class_current =#fill_here\n",
        "\n",
        "        # Récupérer la confiance pour la classe actuelle\n",
        "        confidence_current = score[class_current]\n",
        "\n",
        "        # Vérifier si la confiance dépasse le seuil minimum\n",
        "        if #fill_here:\n",
        "            # Récupérer les coordonnées de la boîte englobante et les ajuster à la taille de l'image originale\n",
        "            box_current = detection[#fill_here:#fill_here] * np.array([w, h, w, h])\n",
        "            x_center, y_center, box_width, box_height = box_current.astype('int')\n",
        "            x_min = int(#fill_here)\n",
        "            y_min = int(#fill_here)\n",
        "\n",
        "            # Ajouter la boîte englobante, la confiance et le numéro de classe aux listes correspondantes\n",
        "            bounding_boxes.append([#fill_here, #fill_here, int(#fill_here), int(#fill_here)])\n",
        "            confidences.append(float(confidence_current))\n",
        "            class_numbers.append(class_current)\n"
      ],
      "metadata": {
        "id": "Ka7mcusEAEEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions :**\n",
        "\n",
        "- Créez les listes bounding_boxes, confidences et class_numbers pour stocker respectivement les boîtes englobantes, les confiances et les numéros de classe associés à chaque détection.\n",
        "- Récupérez les dimensions de l'image en utilisant la méthode shape de l'objet image_input et assignez les valeurs aux variables h et w.\n",
        "- Parcourez les résultats de la sortie du réseau en utilisant une boucle for avec la variable result.\n",
        "- À l'intérieur de la boucle, parcourez les détections dans chaque résultat en utilisant une boucle for avec la variable detection.\n",
        "- Récupérez les scores de confiance pour chaque classe en utilisant la notation de découpage ([5:]) sur la variable detection. Assignez les valeurs à la variable scores.\n",
        "- Trouvez l'indice de classe avec la plus haute probabilité en utilisant np.argmax(scores). Assignez la valeur à la variable class_current.\n",
        "- Récupérez la confiance pour la classe actuelle en utilisant l'indice class_current dans la variable scores. Assignez la valeur à la variable confidence_current.\n",
        "- Complétez la condition if pour vérifier si la confidence_current est supérieure au seuil minimum probability_minimum.\n",
        "- À l'intérieur de la condition, récupérez les coordonnées de la boîte englobante en utilisant la notation de découpage ([0:4]) sur la variable detection. Multipliez ensuite les coordonnées par un tableau de facteurs pour ajuster les coordonnées à la taille de l'image originale. Assignez les valeurs respectivement aux variables x_center, y_center, box_width et box_height.\n",
        "- Calculez les valeurs x_min et y_min en utilisant les variables x_center, y_center, box_width et box_height.\n",
        "- Ajoutez la boîte englobante à la liste bounding_boxes en utilisant la méthode append avec une liste contenant x_min, y_min, int(box_width) et int(box_height).\n",
        "- Ajoutez la confiance à la liste confidences en utilisant la méthode append avec la valeur float(confidence_current).\n",
        "- Ajoutez le numéro de classe à la liste class_numbers en utilisant la méthode append avec la valeur class_current."
      ],
      "metadata": {
        "id": "iLixVgiVw_XV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Suppression des détections redondantes avec la suppression non maximale (NMS)**"
      ],
      "metadata": {
        "id": "WfTON2gLeYiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer les détections redondantes à l'aide de la suppression non maximale (NMS)\n",
        "results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
        "\n",
        "# Vérifier s'il y a des résultats après la suppression non maximale\n",
        "if len(results) > 0\n",
        "    # Parcourir les résultats aplatis\n",
        "    for i in results.flatten():\n",
        "        # Récupérer les coordonnées et les dimensions de la boîte englobante\n",
        "        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
        "        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
        "\n",
        "        # Récupérer la couleur de la boîte englobante pour la classe actuelle\n",
        "        colour_box_current = [int(j) for j in colours[class_numbers[i]]]\n",
        "\n",
        "        # Dessiner la boîte englobante sur l'image d'entrée\n",
        "        cv2.rectangle(image_input, (x_min, y_min), (x_min + box_width, y_min + box_height),\n",
        "                      colour_box_current, 5)\n",
        "\n",
        "        # Ajouter le texte de la classe et de la confiance sur l'image d'entrée\n",
        "        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])], confidences[i])\n",
        "        cv2.putText(image_input, text_box_current, (x_min, y_min - 7), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1.5, colour_box_current, 5)\n"
      ],
      "metadata": {
        "id": "ZdlY_PJnAG2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Configurer la taille de la figure\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "# Afficher l'image convertie en espace de couleur RGB\n",
        "plt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Umv3UBjmAJHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}